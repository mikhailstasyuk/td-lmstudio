### How to Set Up and Use

#### Setup Instructions
1. **Clone the repository** to get started.
2. **Install LM Studio for Windows** to access its features.
3. **Install Anaconda** and create a new Conda environment via the Anaconda Prompt.
4. **Install necessary requirements** for the software to function correctly.
5. In LM Studio, **download your preferred instruct model** for use.
6. **Start the LM Studio Local Inference Server** with your chosen model.
7. **Launch `chat.toe`** to initiate the interface.
8. Within the interface, **find and select the CondaEnv component**. You'll need to enter your Windows username and the name of your Conda environment here.
9. **Activate the environment** by clicking the 'activate' button.
10. **Enter Perform Mode** to start chatting with the model using the UI.

#### To-Do
- [x] Conda environment component created.
- [x] Client for LM Studio Local Server access established.
- [ ] Implement JSON reply parsing.
- [ ] Develop a UI that supports dynamic interactions between user input and model responses.
- [ ] Update the documentation to reflect all changes and improvements.
