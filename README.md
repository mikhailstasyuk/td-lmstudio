### Usage
- Clone this repository.
- Download and install LM Studio for Windows.
- Download and install Anaconda and set up a Conda environment in Anaconda Prompt.
- Install the requirements.
- In LM Studio, download your favorite instruct model.
- Run LM Studio Local Inference Server with the downloaded model.
- Open chat.toe
- Select the CondaEnv component, input your Windows username and the Conda environment name.
- Click activate.
- Enter the Perform Mode, chat with the model using UI.

### TODO
- [x] Create a Conda environment component
- [x] Create a client for accessing an LM Studio Local Server
- [ ] Parsing reply jsons
- [ ] Create a UI with dynamic user input and model reply text
- [ ] Update documentation
